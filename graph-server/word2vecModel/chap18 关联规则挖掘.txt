第18章 关联规则挖掘
　　为什么、是什么、怎么做
18.1 关联规则挖掘概述
　　关联就是反映某个事物与其他事物之间相互依存关系，而关联分析是指在交易数据中，找出存在于项目集合之间的关联模式，即如果两个或多个事物之间存在一定的关联性，则其中一个事物就能通过其他事物进行预测。通常的做法是挖掘隐藏在数据中的相互关系，当两个或多个数据项的取值相互间高概率的重复出现时，那么就会认为它们之间存在一定的关联。

18.1.1一个简单的关联规则应用案例
　　网络购物越来越受欢迎，电商平台的卖家同样竞争激烈。作为电商店铺管理员，我们能够获得每个用户在本店的购买记录，通过利用这些购买记录，采取合适的销售策略，能在一定程度上提高店铺的商品销量，那么如何利用这些购买记录呢？下面我们将一起探索这个问题。
　　表18-2中给出了某电商店铺的部分事务数据，其中包含10个事务，即10条购买记录。我们期望从中发现一些关联规则，从而合理地设置商品组合。
　　表18-1 某电商店铺的事务数据
TID（事务ID）商品列表T100暂略T101T102T103T104T105T106T107T108T109	
　　通过关联规则分析，我们发现了如下几条关联规则：
　　笔记本保护套→机械键盘                          （1）
　　钢笔→保温杯						     （2）
　　手机壳→键盘清洁工具   			       	   （3） 
　　对于关联规则（1），这个推论是可以接受的，购买笔记本保护套的顾客通常是刚买了笔记本，而为了代替笔记本电脑的键盘，这些顾客可能就会再买一个机械键盘。根据这条关联规则，我们可以设置销售策略，顾客在购买机械键盘后少量加价即可赠送笔记本保护套，从而提高销量。
　　对于关联规则（2），这个推论较难直接看出因果关系，但我们可以试着推出钢笔和保温杯的优惠购买套餐，再根据销售情况适时调整。
　　对于关联规则（3），我们可以认为购买手机壳的用户爱好干净整洁，所以会同时购买键盘清洁工具。所以可以在用户挑选手机壳的界面，放入键盘清洁工具的相关图片和链接，刺激消费。
	在下一小节，我们会更详细地介绍关联规则分析所涉及的基本概念，同时也会介绍关联规则的分类，使你对关联规则有更清晰且全面的认识。
	
18.1.2关联规则的定义与分类
　　我们首先需要介绍一些关联规则分析所涉及的基本概念。
　　数据通常以二维表的形式展示，如表18-1所示，每一行都对应一个事务。令T={t_1,t_2,…,t_N}是所有事务的集合。对于每行事务，其中包含的每个商品都称为项。在关联分析中，包含零个或多个项的集合称为项集。如果一个项集包含k个项，则称它为k-项集。例如，{I1，I2，I4}是一个三项集。空集则是不包含任何项的项集。项集有一个重要的属性，称为支持度计数，是指在数据库中包含该项集的事务个数，也称为出现频度。我们用σ(X)表示，具体定义为：
σ(X)=|{t_i |X⊑t_i,t_i∈T}|
　　其中，符号|•|表示集合中元素的个数。
　　关联规则是形如X→Y的蕴涵表达式，其中X和Y是不相交的两个项集。
　　以上述关联规则（1）为例，购买笔记本保护套的顾客也倾向于购买机械键盘，则可以表示如下：
　　I3=>I1[support=40%;confidence=80%]
　　其中support是支持度，可以用于给定数据集的频繁程度，表示所有事务的40%显示机械键盘和笔记本保护套被同时购买；confidence是置信度，可以用于确定后者在包含前者的事务中出现的频繁程度，表示购买笔记本保护套的顾客中80%也购买了机械键盘。
　　对于关联规则X→Y，令N是事务个数，给出支持度（s）和置信度（c）的形式定义如下：
s(X→Y)=(σ(X∪Y))/N
c(X→Y)=(σ(X∪Y))/(σ(X))[l1]
　　为了筛选出有用的关联规则，我们需要根据应用场景设定相应的阈值，分别是最小支持度阈值（minsup）和最小置信度阈值（minconf）。对于项集来说，如果它的支持度满足最小支持度阈值，则称它为频繁项集，频繁k项集的集合通常记为L_k。而对于规则来说，如果它同时满足最小支持度阈值和最小置信度阈值，则称它为强规则。
　　由于在不同的应用场景下，数据形式也会有所不同，挖掘得到的关联规则也会不同。我们基于不同的考量，给出三种常用的关联规则分类，具体如下：
 基于规则所涉及的维数
 单维关联规则：若关联规则中的项或属性只涉及一个维1，则称它为单维关联规则。单维关联规则展示了同一个属性或维内的联系，例如：
buys(X, "laptop")→buys(X, "keyboard")              （18-3）
其中X指代顾客，在该关联规则中，只涉及了buys这一个维，。
 多维关联规则：若关联规则中的项或属性涉及两个及以上的维，则称它为多维关联规则。多维关联规则展示了不同属性或维之间的关联。例如： 
age(X, ""20" -"29\"" )    income(X, ”30K-50K")→buys(X, "laptop") （18-4）
其中X同样指代顾客，该规则涉及三个维，分别是age、income和buys。
 基于规则所处理的值类型
 布尔关联规则：若关联规则只考虑项是否出现，而不涉及具体的取值，则它是布尔关联规则。例如，在关联规则18-3中，它只考虑顾客是否购买laptop和keyboard，而不考虑顾客购买的laptop的价格等属性，所以它是布尔关联规则。
 量化关联规则：若关联规则中还考虑了量化的项或属性之间的关联，则它是量化关联规则。在量化关联规则中，部分项或属性的值被划分为区间，这些属性被称为量化属性。例如在关联规则18-4中，age和income都是量化属性，所以该规则是量化关联规则。注意，量化关联规则中也能包含非量化的项或属性，例如规则18-4中的buys。
 基于所挖掘的数据类型和特征
 时序模式关联规则：在一些应用场景中，关联规则可能涉及到序列特征，即存在基于时间或空间的先后次序。例如：通过店铺订单数据，我们可能发现顾客会先购买PC，再购买机械键盘、显示器等外设，接着购买电脑保护套、键盘清洁泥等小工具。这就是一种序列模式。时序模式关联规则就是包含序列模式信息的关联规则。
 结构模式关联规则：结构模式是指结构数据集中的频繁子结构。与序列相比，结构是一个更一般的概念，包括有向图、无向图、格、树、序列、集合等，单个项可以看作是最简单的结构模式。结构模式关联规则就是挖掘结构模式得到的关联规则。
　　除了上述分类，还有对闭模式、极大模式、不频繁模式、稀有模式、负模式、压缩模式等模式的挖掘而产生的关联规则，我们会在18.6节作简单的介绍，有兴趣的同学也可以自行搜索资料了解。下一小节，我们会向你介绍频繁模式的关联规则挖掘的基本框架，使你对关联规则挖掘的流程有全面的了解。
18.1.3关联规则挖掘的基本框架
　　表18-1 商品ID与商品名称对应关系
　　暂略
　　
　　表18-2 某电商店铺的事务数据

　　关联规则挖掘任务通常分为两个子任务。
　　（1）频繁项集发现：目标是找出所有满足最小支持度阈值的项集，这些项集被称为频繁项集。
　　（2）关联规则产生：目标是从发现的频繁项集中提取出所有满足最小置信度阈值的规则，这些规则被称为强规则。
　　在上述例子中，I={I1, I2, I3, I4, I5, I6, I7}是所有项的集合，其中能产生2^7种项集组合，其中包括1个空集、7个1-项集，21个2-项集，35个3-项集，35个4-项集，21个5-项集，7个6-项集，1个7-项集。我们称这些项集为候选项集。
　　那么怎么判断哪些是频繁项集呢？从频繁项集的定义出发，我们只需要计算出每个项集的支持度，再将其与最小支持度阈值比较，满足的即为频繁项集。要计算候选项集的支持度，我们需要确定候选项集的支持度计数。而确定支持度计数的方法很简单，只需要将候选项集与每个事务进行比较，如果候选项集包含在事务中，就增加它的支持度计数。例如，候选项集{I2, I3}出现在事务T002和T009中，其支持度计数即为2，再根据支持度计算公式，其支持度为0.2。
　　按上述方法计算每个候选项集的支持度，并与最小支持度阈值（本例设为0.3）比较，最后得到14个频繁项集，具体如下：
　　
　　这种频繁项集挖掘方法虽然容易理解，但是计算开销大，需要进行O(NMw)次比较，其中N是事务数，M=2^k-1是候选项集数（不含空集），而w是事务的最大宽度。在本例中，N=10，M=127，w=4。而在实际应用中，这几个值可能很大，所以在下一节我们会介绍更好的算法，降低发现频繁项集的计算复杂度。
　　
　　得到频繁项集后，我们需要从这些频繁项集中提取出关联规则（强规则）。提取的方式可以简要地概括如下：将频繁项集Y划分成两个非空子集X和Y-X，得到候选关联规则X→Y-X，然后计算该候选关联规则的置信度，若满足最小置信度阈值，则该关联规则就是强规则，否则舍弃。注意，X和Y-X必须是非空子集，因为∅→Y和Y→∅没有实际意义。
　　以Y={I1, I3}这个频繁项集为例，从中能够产生2个候选关联规则：{I1}→{I3}，{I3}→{I1}。由于它们的支持度都等于Y的支持度，所以这些候选关联规则必定满足最小支持度阈值。我们需要计算这2个候选关联规则的置信度，根据置信度计算公式，{I3}→{I5}的置信度为(σ({I3,  I5}))/(σ(I3))=3/6=0.5，而{I3}→{I1}的置信度则为(σ({I3,  I5}))/(σ(I5))=3/5=0.6，最后将计算得到的置信度与最小置信度阈值比较，即可得到我们的目标——强规则。
　　每个频繁k-项集能够产生2^k-2个候选关联规则（除去两个包含空集的关联规则），我们首先从上一步得到的频繁项集中产生候选关联规则，然后一一计算它们的置信度并与最小置信度阈值（本例设为0.7）比较，最终就能得到该例下的全部强规则，即我们期望发现的关联规则。在本例中，我们最后能得到6条强规则，具体如下：
　　{I5} → {I6},  confidence=1.0
　　{I6} → {I5},  confidence=1.0
　　{I1, I5} → {I6},  confidence=1.0
　　{I1, I6} → {I5},  confidence=1.0
　　{I3, I5} → {I6},  confidence=1.0
　　{I3, I6} → {I5},  confidence=1.0
　　
　　同样，此处介绍的关联规则产生方法虽然容易理解，但是计算开销较大，我们会在18.3节介绍更好的算法，降低计算复杂度。
　　
　　

18.2 频繁项集发现
　　要降低频繁项集挖掘的计算复杂度，我们可以从以下三个方面进行优化：
 减少候选项集的数目。
 减少候选项集与事务的比较次数。
 减少事务数目。
　　下面，我们将介绍Apriori算法、FP-Tree算法、Eclat算法，并在18.2.4小节中对这几种算法进行比较。
　　
18.2.1 Apriori类算法
　　Apriori算法是第一个关联规则挖掘算法，由Agrawal和R.Srikant于1994年提出，它利用先验原理，开创性地使用了基于支持度的剪枝技术，系统地控制候选项集指数增长。[l2]
　　在介绍Apriori算法之前，我们需要先理解先验原理，它能帮助我们利用支持度度量，减少产生频繁项集时所需扫描的候选项集个数。
　　定理 18.1 先验原理 如果一个项集是频繁的，则它的所有子集一定也是频繁的。
　　为了更好地理解先验原理，我们从一个例子出发。假设项集{I1, I3, I4}是频繁项集，而任何包含项集{I1, I3, I4}的事务一定包含它的子集{I1}，{I3}，{I4}，{I1, I3}，{I1, I4}，{I3, I4}。所以，如果项集{I1, I3, I4}是频繁的，则它的子集的支持度一定大于或等于它本身的支持度，所以它的子集也都是频繁的。
　　相反，如果一个项集是非频繁的，那么它的所有超集也都是非频繁的。例如，假设项集{I1, I3}是非频繁的，则如{I1, I3, I4}，{I1, I3, I5, I6}等{I1, I3}的超集的支持度一定小于或等于{I1, I3}的支持度，所以它的所有超集都是非频繁的。利用这一点，我们就能有效减少候选项集的数目，这种基于支持度度量修剪指数搜索空间的策略称为基于支持度的剪枝。而这种剪枝策略依赖于支持度度量的一个重要性质——反单调性。
　　定义 18.1 反单调性 如果对于项集Y的每个真子集X（即X⊏Y），有f(Y)≤f(X)，那么称度量f具有反单调性。
　　对于支持度度量来说，即一个项集的支持度绝不会超过它的子集的支持度。而除了支持度度量，一些其它度量也具有反单调性，同样能够应用在挖掘算法中，以有效地修剪候选项集的指数搜索空间。[l3]
　　了解了先验原理，下面我们将结合具体实例来介绍Apriori算法。
　　Apriori算法的基本思想是通过逐层搜索迭代，用上一轮迭代得到的k项集来探索下一轮迭代的（k+1）项集。初始时，我们将每个项都看作候选1-项集，其集合记为C_1，对它们进行支持度计数后，满足最小支持度阈值的项集被留下，构成频繁1-项集，其集合记为L_1。在第二轮迭代中，利用L_1来生成候选2-项集的集合C_2，然后利用先验原理进行剪枝，只保留那些所有子集都频繁的候选2-项集，再对这些候选2-项集进行支持度计数，与最小支持度阈值比较，得到频繁2-项集的集合L_2，在下一轮迭代，再以同样方法找出L_3，如此迭代下去，直到不能再发现频繁k项集为止。
　　表18-2 某电商店铺的事务数据
　　
　　以18.1.3小节中的数据为例，共有10个事务，设定最小支持度阈值为0.3，即最小支持度计数为3。
　　第一轮迭代：每个项都是候选1-项集的集合C_1的成员，算法扫描全部事务，对每个候选1-项集的出现次数计数。将每个候选项集的支持度计数与最小支持度计数比较，得到频繁1-项集的集合L_1，在本例中，{I4}的支持度计数不满足最小支持度计数，被剪去。
　　
图18-1 第一轮迭代过程
　　第二轮迭代：通过L_1⋈L_1来产生候选2-项集的集合C_2，共有15个2-项集，由于它们的所有子集都是频繁的，所以没有候选被删除。然后对C_2中的每个候选进行支持度计数。将每个候选项集的支持度计数与最小支持度计数比较，得到L_2。

图18-2 第二轮迭代过程
　　第三轮迭代：通过L_2⋈L_2来产生候选3-项集的集合C_3，共有4个3-项集，具体为C_3=L_2⋈L_2={{I1, I3, I5}, {I1, I3, I6}, {I1, I5, I6}, {I3, I5, I6}}。根据先验原理，这四个候选项集的所有子集都是频繁的，所以这四个项集的集合构成C_2。然后对C_2中的每个候选进行支持度计数，将每个候选项集的支持度计数与最小支持度计数比较，得到L_3。

图18-3 第三轮迭代过程
　　第四轮迭代：通过L_3⋈L_3来产生候选4-项集的集合C_4，其中只有一个候选4-项集{I1, I3, I5, I6}，而根据先验原理，它不满足所有的子集都是频繁的条件，所以C_4=∅，因此算法终止，我们成功得到了全部频繁项集。
　　
　　下面给出Apriori算法产生频繁项集的伪代码。
　　
　　下面分析该算法的复杂度。（与穷举法比较）[l4]
　　
　　那么如何才能进一步提高Apriori算法的效率呢？下面，我们将简单介绍一些经过实践验证且比较有效的优化方法。
　　基于散列优化支持度计数 首先，需要设置一个合理的散列函数，当散列k-项集时，散列函数就有k个自变量。如散列2-项集时，可以设散列函数如下：
　　h(x, y)=(f(x)*10+f(y))  mod 7
　　其中x是项，f(x)是一个自定义的映射，将项映射到一个数值，比如f(I_k )= k。当需要从L_(k-1)生成C_k时，我们可以扫描一次全部事务，然后生成全部候选k-项集，将它们按照散列函数散列到不同的桶中，统计每个桶的项集个数。如果一个桶的项集个数低于最小支持度阈值，则这个桶内的全部项集都可以排除，从而减少了候选k-项集的数量。经过实践，当k=2时，优化效果尤为明显。
　　压缩迭代时扫描的事务数 如果一个事务，不包含任何一个频繁k-项集，那么这个事务也一定不包含任何一个频繁（k+1）-项集。因此，在产生频繁k-项集的集合L_k后，如果一个事务满足这一点，就可以将其删除或标记，在之后的迭代中不再需要考虑。
　　基于划分快速寻找频繁项集 划分技术是分治思想的体现。首先，我们将全部事务尽量均匀地划分为n个分组，然后根据最小支持度阈值，找出每个分组中的频繁项集，称为局部频繁项集。接着，我们将全体局部频繁项集作为候选项集，再次扫描全部事务，计算每个候选项集的实际支持度，与最小支持度阈值比较后，得到全局频繁项集。值得一提的是，如果每个分组的大小都能放入内存，那么运行效率就会较快，我们可以依次确定分组的数量。
　　抽样搜索频繁项集 首先，我们从全部事务中随机抽取部分事务集合S，然后在S中寻找频繁项集，通常会控制S的大小，使其能装入主存，使得扫描一遍即可完成计算。显而易见，这种方法虽然减少了事务数，但是会遗漏部分频繁项集。为了降低这种可能性，我们在S中寻找频繁项集时，设定更小的最小支持度阈值，找到S中的频繁项集后，我们使用一种机制来确定是否有遗漏，若有遗漏，则需要对全部事务进行第二次扫描，反之，就结束搜索。这种方法通常应用在效率优先的场合。

18.2.2 FP-tree算法
　　这一小节我们将介绍一种完全不同于Apriori算法的频繁项集挖掘方法，这种方法由韩嘉炜等人于2000年提出，名为频繁模式增长（Frequent-Pattern Growth），它通过使用一种紧凑的数据结构——FP树（FP-tree）来组织数据，并从中发现频繁项集，所以也被称为FP-tree算法。
　　FP-tree算法的基本思想是依次读入每个事务，构造出FP-tree，然后自底向上探索FP-tree的全部路径，查找出全部频繁项集。这个方法也可以分为两步，首先是构造出FP-tree，然后是由FP-tree产生频繁项集。
　　同样以18.1.3节中的数据为例，设定最小支持度阈值为0.2，即最小支持度计数为2。
　　第一次扫描全部事务，得到每个1-项集的支持度计数，与最小支持度计数比较后，{I4}被剪枝，剩余的构成频繁1-项集的集合，并将它们按支持度递减排序。
　　第二次扫描，就开始构建FP-tree。第一步，创建根结点，标记为“null”。然后读入第一个事务{I1，I2}，构造这两个项的结点，并按照支持度递减的次序处理，形成null→I1→I2的路径，路径上结点的计数增加1，初始为0。接着读入第二个事务{I2，I3，I5，I6}，需要创建I2、I3、I5、I6的结点，形成null→I3→I5→I6→I2的路径，这四个结点的计数增加1。同理读入第三个事务{I3，I5，I6}。再读入第四个事务{I1，I3，I7}，由于该事务的第一个项也是I1，所以只需创建I3和I7的结点，该事务的路径null→I1→I3→I7与第一个事务的路径null→I1→I2部分重叠。读入前4个事务的FP-tree的构建过程如图18-4所示。
　　继续读入剩余的6个事务，按上述步骤依次处理，最后便能得到完整的FP-tree。为了使遍历FP-tree更加快捷，我们将频繁1-项集的集合与FP-tree链接，通过结点链，使每个项都能在FP-tree中快速定位，最后得到的FP-tree如图18-5所示。建议同学们自己动手尝试一下，从而更好地理解FP-tree的构建过程。
　　
图18-4 依次读入前4个事务的FP-tree
　　
　　
图18-5 读入全部事务的FP-tree
　　
　　得到FP-tree后，我们就可以在其中进行频繁模式挖掘了。首先，从全局来看，我们可以将全体频繁项集的集合分为多个小集合，在本例中，可以将全体频繁项集的集合划分为分别以I1、I2、I3、I5、I6、I7结尾的频繁项集集合，其中每个频繁项集中的项都按照支持度递减排序，从而避免重复。在这里，我们利用了分治的思想，将一个规模较大的问题转化为了6个规模较小的问题。下面，我们就来解决挖掘以某个项结尾的频繁项集的子问题。
　　我们首先考虑挖掘以I7结尾的频繁项集集合，因为I7的支持度最低，按支持度由低到高来解决这6个子问题是有理由的，我们之后会进行解释，暂且先将注意力集中到这个子问题上。观察图18-5中的FP-tree，根据结点链，我们发现I7出现在3个分枝中，对应的路径分别是null→I1:6→I3:3→I7:1、null→I3:3→I2:1→I7:1和null→I2:1→I7:1。{I7}本身是频繁项集，再考虑以I7为结尾，有I1:6→I3:3、I3:3→I2:1和I2:1三条前缀路径。由于这些路径中存在某些事务并不包含I6，所以我们要将那些事务去除，更新前缀路径上的支持度计数，即得到I1:1→I3:1、I3:1→I2:1和I2:1三条前缀路径，将这三条前缀路径上的项以及它们的支持度计数看作事务数据库中的数据，可以得到“子事务数据库”，也称为I6的条件模式基。再根据条件模式基来构造FP-tree，将其称为I6的条件FP-tree，如图18-6所示。
　　
图18-6  I7的条件FP-tree
　　由于I7的条件FP-tree中，所有可能的前缀项集的支持度计数都小于最小支持度阈值3，所以我们不必继续探索，可以得出结论：除去{I7}本身外，不存在以I7结尾的频繁项集。（此后，我们不再指出除去结尾本身的频繁项集这一点）
　　然后我们要解决下一个子问题，即挖掘以I2结尾的频繁项集，因为I2的支持度次低。以同样的步骤得到I2的条件模式基，并构建I2的条件FP-tree，如图18-7所示。
　　
图18-7  I2的条件FP-tree
　　同样，I2的所有可能前缀项集的支持度计数也都小于3，所以可以得出结论：不存在以I2结尾的频繁项集。
　　接着，我们挖掘以I6结尾的频繁项集，以同样的步骤可以得到I6的条件FP-tree，如图18-8所示。
　　
图18-8  I6的条件FP-tree
　　此时，我们发现，有3个前缀项集的支持度满足最小支持度阈值，所以我们需要继续探索。让我们再次利用分治思想，挖掘以I6结尾的频繁项集的问题可以再次分解为3个子问题，分别是挖掘以{I1, I6}、{I3, I6}、{I5, I6}结尾的频繁项集。同样，以支持度从低到高依次解决每个子问题。首先是挖掘以{I3, I6}结尾的频繁项集，{I3, I6}的支持度计数为3，满足最小支持度计数，所以是频繁2-项集，然后我们按照与之前相同的方式寻找出{I3, I6}的条件模式基，并构建对应的条件FP-tree。如图18-9（1）所示。由于其前缀项集支持度计数小于3，所以不存在以{I3, I6}结尾的频繁项集。接着是挖掘以{I1, I6}结尾的频繁项集，{I1, I6}的支持度计数为3，所以是频繁2-项集。而由于其条件FP-tree为空，所以并未画出，结论是不存在以{I1, I6}结尾的频繁项集。
　　最后是挖掘以{I5, I6}结尾的频繁项集，{I5, I6}的支持度计数为5，所以是频繁2-项集。其条件FP-tree如图18-9（2）所示。由于其前缀项集{I1}和{I3}的支持度计数都满足最小支持度计数，所以我们可以再将问题分解为两个子问题，分别是挖掘以{I1, I5, I6}和{I3, I5, I6}为结尾的频繁项集。这两个子问题已经十分清晰，我们便不再画出其条件FP-tree，而是直接给出结果。{I1, I5, I6}和{I3, I5, I6}的支持度计数都是3，所以都是频繁3-项集。而以{I3, I5, I6}结尾的候选4-项集只有{I1, I3, I5, I6}，其支持度计数为1，不满足最小支持度计数，以{I1, I5, I6}结尾的4-项集不存在。至此，挖掘以I6结尾的频繁项集，该子问题求解结束。
　　
图18-9  左为I3, I6的条件FP-tree，右为I5, I6的条件FP-tree
　　当然，我们还需要将三个子问题的求解结果合并，得到集合{{I1, I5, I6}，{I3, I5, I6}，{I5, I6}，{I3, I6}，{I1, I6}}，这就是通过FP-tree挖掘得到的以I6结尾的所有频繁项集（不包括{I6}这个频繁1-项集）。
　　接着，按照算法思想，我们继续依次求解以I5、I3、I1结尾的频繁项集集合，结果如表18-3所示。最后，将FP-tree挖掘结果与频繁1-项集的集合L_1合并，就得到了我们所要挖掘的全体频繁项集，不难发现，这与18.2.1节中Apriori算法的挖掘结果是相同的。
　　表18-3 FP-tree挖掘结果

　　下面我们给出FP-tree算法产生频繁项集的伪代码。建议同学们结合伪代码再次体会FP-tree算法的思想。
　　
　　
　　
　　
　　
　　
　　
　　
　　

　　现在我们来解释一下为什么按支持度由低到高来解决子问题，由于每次选择的是最不频繁的项作为结尾，在搜索频繁模式时能够拥有较好的选择性，即有较多的条件模式基，相应的条件FP-tree也更“茂盛”，我们观察表18-3，可以发现，由上至下，结尾项的支持度是递减的，但是条件模式基的数量却是递增的，这就是较好的选择性的含义。FP-tree算法的有趣之处在于压缩事务数据库，递归生成子问题，从而快速搜寻频繁项集。不过，当事务数据库较大时，FP-tree也会较大，无法一次性放入主存，导致效率降低。这时，我们可以将事务数据库划分成多个投影数据库，并在每个投影数据库上应用FP-tree算法，提高挖掘效率。
　　下面我们就来介绍一下如何得到投影数据库并应用FP-tree算法。
　　让我们从数据库投影的过程开始介绍。首先按照支持度从小到大的顺序，从频繁1-项集取出支持度最小的项t，然后将原来的事务数据库中包含t的事务取出，去除这些事务中的t和非频繁项，就得到了原数据库到项t的投影数据库。然后，从频繁1-项集中取出下一个项p，同样将原来的事务数据库中包含p的事务取出，并去除这些事务中的p、t和非频繁项，从而得到了原数据库到项p的投影数据库。注意，某一项的投影数据库中，不会包含非频繁项、该项自身、以及按支持度从大到小排序好的频繁1-项集中位序比该项靠后的项。之后，只要以相同的方式处理频繁1-项集中的每个项，我们就将原本的事务数据库划分为了多个投影数据库。
　　以18.1.3节中的数据为例，假设频繁1-项集的排序为I1-I3-I5-I6-I2-I7，我们得到6个投影数据库如下：
　　表18-3 投影数据库
　　
　　可以发现，对频繁项t，其投影数据库中都不包含支持度排序在其之后的项和它自身。表中第三列为按支持度计数筛选后的条件模式基。得到全部投影数据库后，我们只需要在其中条件模式基不为空的投影数据库应用FP-tree算法，找到每个投影数据库上的频繁项集，最后将结果合并，即可得到全部频繁项集。细心的同学可能已经发现，投影数据库生成的FP-tree和之前原算法生成的条件FP-tree十分相似，实际上，这两个过程本就是同源的。
　　该投影过程只需要扫描一遍数据库，虽然在介绍的时候，我们是依次投影得到各个投影数据库，但实际上，这些投影数据库可以并行生成，并不会互相影响，因此，这个投影方法也被称为并行投影（parallel projection）。
　　研究证明，投影后的子数据库所生成的FP-tree在大小上通常比原数据库所生成的FP-tree小几个数量级。因此，通过构造投影数据库，能够使FP-tree算法有效地应对大型数据库的应用场景。此时，你可能还会问，那如果投影数据库依旧不能一次性放入主存，该怎么办？一个不难想到的解决方案是继续投影，递归地执行这一过程，直到最终得到的投影数据库能够一次性放入主存为止。
　　不过，并行投影在实际应用中，依旧可能遇到一些问题。假设一个事务数据库中的某个事务长度很大且包含很多频繁项，那么在构造到每个频繁项的投影数据库的过程中，这个事务就会被重复投影多次。如果该事务中含m个频繁项，那么就会被投影到m-1个投影数据库，该事务就会占据大小约为∑_(i=0)^(m-1)▒i=(m(m-1))/2的空间，而原先该事务只需要占据大小约为m的空间，投影后所占据的空间是原先的(m-1)/2倍，这在许多时候是十分浪费且难以接受的。
　　为了克服这个缺点，我们可以使用另一种投影方法——分区投影（partition projection）。分区投影的执行过程和并行投影略有不同，下面我们进行介绍。对于事务数据库中的每个事务，都只投影到项t的投影数据库，其中项t是该条事务中支持度排序最靠后的频繁项。而具体的投影规则和并行投影一样，即当投影到项t时，不包含非频繁项、支持度排序在项t之后的频繁项和项t自身。这样，扫描一遍事务数据库后，我们得到了初始的投影数据库。接着，我们先在支持度最小的投影数据库上应用FP-tree算法，在本例中，即先处理到I7的投影数据库，因为其初始数据信息是完整的。处理完到I7的投影数据库后，我们需要将该投影数据库中的每个事务继续按上述规则进行投影，以确保下一个被处理的投影数据库也拥有完整的数据信息，从而避免遗漏。如图所示，处理完到I7的投影数据库后，需要将第一个事务{I1, I3}继续投影，投影到该事务中支持度最小的I3的投影数据库上，得到{I1}；第二个事务{I2}只有一个项，无需投影，因为投影到I2上也只是空集；第三个事务{I2, I3}则需要投影到I2的投影数据库上，得到{I3}。然后我们便可以处理到I2的投影数据库，挖掘结束后，同样需要将每个事务继续投影，以确保之后处理的数据库拥有完整的数据信息。具体过程不再赘述。当全部投影数据库处理完后，合并挖掘结果，便得到所有的频繁项集。
　　
图18-10  分区投影过程
　　在分区投影中，投影数据库的总大小小于原始数据库，克服了并行投影中空间浪费的缺点，并且，它通常只需要较少的内存和I/O操作。不过，分区投影必须以特定的顺序依次处理这些投影数据库，并且需要进行多次投影。但总体来说，分区投影的内存需求较低，更适合在实际应用中使用。
　　
18.2.3 Eclat算法
　　这一节我们将介绍Eclat（Equivalence CLAss Transformation）算法，也叫做等价类变换算法。它与前面介绍的两个算法不同，它使用垂直数据格式来挖掘频繁项集，而Apriori算法和FP-tree算法都是使用水平数据格式来挖掘频繁项集。那么什么是水平数据格式和垂直数据格式呢？下面我们以具体的例子来表述。
　　如表18-2所示的数据格式为水平数据格式，即形如{TID：itemset}的数据格式，其中第一列为事务标识号，第二列为事务涉及的项集。而如下表18-4所示，形如{itemset：TID}的数据格式，被称为垂直数据格式，即第一列为项集，第二列为包含该项集的事务标识号的集合。
　　表18-4 垂直数据格式表示的事务数据库
　　
　　
　　理解了垂直数据格式，我们来介绍Eclat算法的基本思想。和Apriori算法类似，Eclat算法也是通过逐层迭代来产生频繁项集，即通过频繁k-项集的集合L_k来生成L_(k+1)，其中同样利用了18.2.1小节中介绍的先验原理。但是挖掘的数据格式不同，每次迭代产生频繁项集的过程自然也不同。在垂直数据格式中，项集的支持度很容易得到，即TID集的长度。得到频繁k-项集后，我们对TID集取交集，对项集取并集，再计算对应的（k+1）项集的支持度，与最小支持度阈值比较，就可以得到频繁（k+1）-项集的集合L_(k+1)。如此继续下去，k逐渐递增，直到不再产生频繁项集为止。
　　下面我们就以表18.2的数据为例，设定最小支持度计数为3，来介绍Eclat算法的执行流程。第一步，扫描一次事务数据库，将水平格式数据转换为垂直格式，得到表18-4所示数据。对每行项集计算TID长度，即为相应的支持度计数，与最小支持度计数比较，得到频繁1-项集的集合L_1。然后，对全部频繁项集两两求并集，同时对它们的TID集两两求交集，得到垂直数据格式的2-项集。
　　表18-5 垂直数据格式表示的2-项集
　　
　　根据表18-5很容易得到每个2-项集的支持度，然后与最小支持度计数比较后，得到频繁2-项集如表18-6所示。接着，我们要通过L_2来生成L_3。由于并非所有的2-项集都是频繁项集，这时先验原理就派上用场了。根据先验原理，只有每个2-项集子集都是频繁项集的3-项集才可能是频繁3-项集。这样，我们得到四个候选3-项集，即{I1, I3, I5}, {I1, I3, I6}, {I1, I5, I6}, {I3, I5, I6}，对每个候选3-项集，取它的任意两个2-项集，对项集作并运算，对TID集作交运算，结果如表18-7所示。
　　表18-6 垂直数据格式表示的频繁2-项集
　　
　　表18-7 垂直数据格式表示的3-项集
　　
　　其中项集{I1, I5, I6}和{I3, I5, I6}满足最小支持度计数，所以是频繁3-项集，它们构成了L_3。然后我们要通过L_3来生成L_4，其中只有一个候选4-项集，即{I1, I3, I5, I6}，其支持度计数为1，故剪去。最后，由于L_4为空集，所以无法再构造5-项集，算法执行结束，我们也找到了全部的频繁项集，和前两个算法得到的结果相同。
　　下面我们给出Eclat算法的伪代码实现，如下：
　　
　　使用垂直数据格式的优点很明显，在计算项集的支持度时，不需要扫描事务数据库，只需要计算TID集的长度即可。而且在由L_k计算L_(k+1)时，利用先验原理，剪枝效果很好，能有效降低候选项集数量。但是，当TID集长度较大时，其交运算的开销较大，且会占用大量内存空间。
　　想要对Eclat算法的上述缺点进行优化，我们可以采用差集技术。差集顾名思义，就是两个集合之差，而我们可以通过记录（k+1）-项集与相对应的k-项集之间的TID集的差集，如上例中，{I1}的TID集为{T001, T004, T005, T007, T008, T010}, {I1, I3}的TID集为{ T004, T007, T008}，两者的差集即为{T001, T005, T010}，这样只需记录差集，而不必完全存储两个TID集。相关的优化算法有GenMax算法，它利用差集进行快速频繁项集检验，降低TID集的存储开销和交运算的计算开销。当数据集较稠密或数据集中包含较多长模式时，差集技术的作用尤为明显。[l5]
18.2.4优缺点讨论
　　现在，相信你对Apriori算法、FP-tree算法和Eclat算法的原理已经比较熟悉了，下面我们将从不同角度对这三种算法的标准实现（不考虑各种优化技术）进行比较，带你一起梳理它们各自的优势和缺陷，以及适合的应用场景，使你对它们的了解更加全面清晰。
　　Apriori算法原理简洁，易于实现，十分适合稀疏数据集中的频繁模式挖掘。而当待挖掘的数据量较大时，虽然与穷举法相比，Apriori算法有显著的性能提高，但是其速度仍不能令人满意，且其空间复杂度较高，尤其是当L_1较大时，L_2的数量也会很大，每次迭代搜索下一层频繁项集时，它都重新需要扫描一遍数据集，从而产生不可忽视的I/O开销。
　　FP-tree算法使用一个高度压缩的数据结构存储了事务数据库的信息，整个过程只需扫描两次数据集，相关研究表明，在挖掘某些事务数据集时，FP-tree算法比Apriori算法快多个数量级。不过，由于FP-tree算法在执行过程中需要递归生成条件数据库和条件FP-tree，所以内存开销也较大，且当生成的FP-tree十分茂盛时，如满前缀树，算法产生的子问题数量会剧增，导致性能显著下降。需要注意的是，FP-tree算法和Apriori算法都只能用于挖掘单维的布尔型关联规则。
　　Eclat算法是一种深度优先算法，利用了倒排思想，只需要扫描一次原始数据集。它的性能主要取决于垂直数据格式表示下的TID集长度，当TID集较短时，算法性能不逊于FP-tree算法。而当TID集很长时，不仅需要大量存储空间，其交运算耗时也会随之增加，导致算法效率下降。
　　随着数据环境日益复杂，上述算法很难直接应用，但它们背后的思想仍然值得我们学习，对我们理解频繁项集挖掘问题有重要的帮助。
　　
18.3 关联规则生成及相关性评价
　　从频繁项集里提取关联规则的过程中，可能产生大量关联规则，而其中很大一部分是人们所不感兴趣的，且某项关联规则是否有趣也会因人而异，如何建立有效且广泛适用的关联规则评估标准是一个重要且有趣的问题。
　　下面，我们将介绍从频繁项集产生关联规则的过程，并对现有的多种关联规则评估方法进行阐释和比较。
18.3.1候选关联规则生成
　　前面我们介绍了三种频繁项集挖掘方法，而得到频繁项集后，下一步就是根据这些频繁项集来产生关联规则，并从中筛选出强关联规则，即满足最小支持度阈值和最小置信度阈值的关联规则。在18.1.3小节中，我们使用了枚举法来产生关联规则，介绍了产生关联规则的基本过程，下面我们将更详细地介绍这一过程及相关改进。
　　让我们回忆一下从频繁项集产生关联规则的原理：首先将频繁项集Y划分成两个非空子集X和Y-X，得到候选关联规则X→Y-X，然后计算该候选关联规则的置信度，若满足最小置信度阈值，则该关联规则就是强关联规则，否则舍弃。其中，X和Y-X必须是非空子集，因为∅→Y和Y→∅没有实际意义。
　　以从表18-2数据中挖掘到的频繁3-项集为例，令Y={I3, I5, I6}，我们可以得到如表18-8前三列所示6种不同的划分。
　　表18-8 由频繁项集Y产生的候选关联规则
　　
　　然后依次计算每个候选关联规则的置信度，如表18-8所示，而我们设定的最小置信度阈值为0.70，所以我们能从该频繁3-项集中得到2条强关联规则，分别是{I3, I5}→{I6}和{I3, I6}→{I5}。
　　对每个频繁k-项集（k≥2）执行上述过程，就能得到全部强关联规则。而每个频繁k-项集能产生2^k-2个候选关联规则，不仅占用大量内存空间，也会导致计算过程耗时较长，下面，我们来介绍一种有效的剪枝方法，从而减少候选关联规则数量，提高效率。
18.3.2候选关联规则的剪枝
　　在生成频繁项集的过程中，我们利用支持度度量的反单调性，即先验原理，有效减少了候选项集的数量。那么在候选关联规则的剪枝中，能否利用置信度度量进行类似的剪枝呢？要考察这一点，我们首先要尝试证明置信度度量是否同样具有反单调性。假设存在一条规则X→Y，其置信度为c_1，则令x⊑X，y⊑Y，从而得到规则x→y，其置信度为c_2。若置信度度量满足反单调性，则c_2≥c_1须恒成立。遗憾的是，c_2与c_1之间的大小关系是不确定的，大于、小于和等于都有可能。
　　表18-8 置信度度量不满足反单调性的案例
　　
　　幸运的是，置信度度量满足定理18.2，能够帮助我们利用它进行剪枝，具体如下：
　　定理 18.2  令Y是一个项集，且X⊑Y。如果规则X→Y-X不满足置信度阈值，则形如T→Y-T的规则也一定不满足置信度阈值，其中T⊑X。
　　该定理很容易证明，由于T⊑X，所以σ(T)≥σ(X)，而规则T→Y-T的置信度和规则X→Y-X的置信度分别是(σ(Y))/(σ(T))和(σ(Y))/(σ(X))，故前者的置信度不可能大于后者。
　　那么下面我们就来介绍如何利用置信度度量的这个性质进行候选关联规则的剪枝。
　　类似于Apriori算法，我们通过逐层迭代来产生关联规则，关联规则后件中的项数随层数递增。同样以频繁3-项集{I3, I5, I6}为例，如图18-8所示，该频繁3-项集能产生6条候选关联规则，其中三条规则的后件数为1，另外三条的后件数为2。
　　
图18-11  使用置信度度量对关联规则进行剪枝
　　其中后件数为1的三条关联规则中，规则{I5, I6}→{I1}的置信度不满足最小置信度阈值，则剪去该关联规则，由剩余的2条强关联规则生成新的候选规则，具体过程是合并这两条规则的后件，从而产生下一层的关联规则{I3}→{I5, I6}。在图18-8中，我们绘出了全部的关联规则，实际上，当规则{I5, I6}→{I1}被剪去时，就不需要再由其产生下一层关联规则，相当于直接剪掉了从该结点生成的全部子关联规则，即图18-8中虚线圈起来的部分，这样原本的6条候选关联规则就剪去了3条。当k值更大时，对于频繁k-项集的剪枝效果会更明显，能有效减少候选关联规则的数目。
　　现在，我们来概括一下由频繁项集生成关联规则的过程。首先，对每个频繁k-项集（k≥2）按上述方式逐层构造关联规则，计算它们的置信度，若某条关联规则不满足最小置信度阈值，则在下一轮迭代中，不再考虑该关联规则，由剩余的强关联规则来产生下一层的候选关联规则，直到产生的规则中后件的项数等于k-1时即可停止迭代，此时我们就得到了全部的强关联规则。
　　到此，我们的任务还没有结束。并非所有的强关联规则都是有趣的，甚至有些强关联规则是有误导的，下一节，我们将介绍更加客观实用的规则评估方法，以帮助我们进一步筛选到我们所需要的、有意义的强关联规则。
18.3.3相关性评价指标
　　对关联规则是否有趣的评估方法总体可以分为两类，第一类是基于数据驱动的客观兴趣度度量，第二类则是通过主观论据建立的主观兴趣度度量。由于主观兴趣度度量通常因用户而异，若要将其加入规则评估方法中，需要来自对应领域的专家经验与判断，比较困难，所以我们这一小节的重点是客观兴趣度度量，它能利用统计方法提供有效的评估手段，帮助我们去除无趣的关联规则。
　　在频繁项集挖掘和关联规则生成的过程中，我们用到了支持度度量和置信度度量，这两者都是客观度量，通常称为支持度-置信度框架，它能帮助我们做出一些筛选，但是它同样具有不小的局限性，基于该框架，我们可能会疏忽一些潜在的有意义的关联模式，也可能得到一些有误导的强关联规则，下面我们举例说明。
　　表18-9 关于喝绿茶与喝咖啡的千人统计
　　[l6]
　　假设最小支持度阈值和最小置信度阈值分别是10%和70%。
　　令I1=爱喝绿茶，I2=爱喝咖啡。考察关联规则{I1}→{I2}，其支持度为15%，置信度为75%，按照支持度-置信度框架评估，则该规则是强关联规则。然而，该关联规则具有误导性。在全体测试人群中，爱喝咖啡的人占比达80%，而其中同时也爱喝茶的人只占75%，即若一个人爱喝茶，则他爱喝咖啡的可能性反而从80%下降到了75%，这是一个有误导性的强规则，所以，我们需要寻找更可信的评估度量方法。
　　第一种方法是使用相关性度量对基础的支持度-置信度框架进行扩充，即在原来的度量基础上考虑关联规则的前件和后件之间的相关性，所以关联规则的表示形式将有所变化，具体如下：
　　X→Y [ support, confidence, correlation ]
　　但相关性度量有多种方式，这一小节我们将介绍提升度和和和^2两种相关性度量方式。
　　首先介绍提升度。令P(X)表示事件X发生的概率。假设有前件A和后件B，若满足P(A∪B)=P(A)×P(B)，则前件A和后件B是统计独立的，即A的发生和B的发生是互不影响且没有联系的，将P(A)×P(B)的值记为P_std。若不满足，则称A与B是依赖的或相关的，两者同时发生的概率即为P(A∪B)。A发生和B发生之间的提升度就可以定义如下：
　　lift(A, B)=(P(A∪B))/P_std =(P(A∪B))/(P(A)×P(B))
　　该定义很容易推广到两项以上的情况，如下：
　　lift(X_1, X_2,…,X_n )=(P(X_1∪X_2∪…∪X_n))/(P(X_1 )×P(X_2 )×…×P(X_n))
　　下面对提升度进行简单的解释。
　　lift(A, B){█(=1，A和B是独立的@>1，A和B呈正相关@<1，A和B呈负相关)┤
　　负相关是指A的发生可能导致B不发生，所以提升度小于1的关联规则通常会被拒绝。让我们重新考察上述咖啡和绿茶的案例。关联规则{I1}→{I2}的提升度为0.15/(0.2×0.8)=0.9375，说明前件和后件之间呈负相关，所以该关联规则会被拒绝。
　　回顾置信度的计算公式，不难发现，置信度度量只考虑了P(A∪B)和P(A)之间的偏差，而提升度则考虑的是P(A∪B)和P_std之间的偏差，所以置信度不能解释后件的支持度，从而会导致产生出有误导性的关联规则的情况。
　　下面我们将介绍使用下^2分析来度量相关性。首先，对关联规则X→Y，我们假设两者统计独立，从而计算出期望值，而原统计值则称为观测值，从而计算，^2值如下：
　　　^2=∑▒〖(观测值-期望值)〗^2/期望值
　　　^2值能够反映理论值和实际观测值的差异程度，得到得^2值后，我们不能像提升度那样直接与1比较，从而得出结论，而是需要查询比^2分布的临界值表，然后再根据用户设定的显著性水平α（通常为0.05），判断是否拒绝该关联规则。在此我们不作详细描述，有兴趣的同学可以参考…[l7]。
　　同样重新考察上述咖啡与绿茶的案例。如果喝绿茶与喝咖啡是统计独立的，那么按照表18-9中的数据，爱喝咖啡的期望人数占比为(650+150)/1000=0.8，那么爱喝绿茶的人中还爱喝咖啡的人数的期望值就是200×0.8=160。同理，按照期望占比重新计算该表格数据后，得到对应的期望值表格如下：
　　表18-9 关于喝绿茶与喝咖啡的千人统计
　　
　　然后按照然^2值计算公式，计算如下：
　　　^2=〖(150-160)〗^2/160+〖(50-40)〗^2/40+〖(650-640)〗^2/640+〖(150-160)〗^2/160=3.90625
　　查阅查^2分布的临界值表，两者以95%的概率不相关的临界值是3.84，由于，^2值大于该临界值，所以爱喝绿茶和爱喝咖啡之间是有关联的，且（爱喝绿茶，爱喝咖啡）处的观测值150小于期望值160，说明两者是负相关的，与使用提升度得到的分析结果相同。
　　除此之外，还有其他的评估度量方式值得我们了解。下面我们将介绍四种具有零不变性的度量：全置信度、最大置信度、Kulczynski和余弦。
　　首先介绍全置信度，两个项集A和B的全置信度定义如下：
　　all_confidence(A, B)=(sup⁡(A∪B))/(max⁡{sup⁡(A), sup⁡(B)})
　　全置信度计算公式的分子是项集A和B同时出现的支持度，分母则是项集中的最大支持度，由于支持度度量具有反单调性，所以sup⁡(A∪B)≤min⁡{sup⁡(A), sup⁡(B)}，即全置信度是项集{A, B}产生的关联规则A→B和B→A中的置信度下界，这也是全置信度的特点，即：
　　all_confidence(A, B)≤(min⁡{sup⁡(A), sup⁡(B)})/(max⁡{sup⁡(A), sup⁡(B)})
　　值得注意的是，虽然置信度度量不具有反单调性，但是全置信度度量是具有反单调性的，所以可以直接在前述的挖掘算法中用于剪枝策略。
　　然后是最大置信度，两个项集A和B的最大置信度定义如下：
　　max_confidence(A, B)=(sup⁡(A∪B))/(min⁡{sup⁡(A), sup⁡(B)})
　　与全置信度计算公式略有不同，最大置信度计算公式的分母是项集中的最小支持度，所以它的含义是项集{A, B}产生的关联规则A→B和B→A的最大置信度。
　　下一个是Kulczynski（Kulc）度量，由波兰数学家S. Kulczynski于1927年提出。项集A和B的Kulc度量定义如下：
　　Kulc(A, B)=1/2[conf(A→B)+conf(B→A)]=1/2[sup⁡(A∪B)/sup⁡(A) +sup⁡(A∪B)/sup⁡(B) ]
　　由计算公式不难看出，Kulc度量是项集{A, B}产生的关联规则A→B和B→A的各自置信度的平均值。
　　最后是余弦度量，项集A和B的余弦度量定义如下：
　　consine(A, B)=(sup⁡(A∪B))/√(sup⁡(A)×sup⁡(B))=(P(A∪B))/√(P(A)×P(B))
　　细心的同学可能注意到了余弦度量和提升度的计算公式十分相近，只是余弦在分母处多了取平方根的计算。所以，余弦度量也称为调和提升度度量，而通过取平方根，余弦度量消除了事务总个数的影响，而只会受到A、B、A∪B的支持度的影响。
　　实际上，上述前三种度量也具备余弦度量的这个性质，即度量值不受事务总个数的影响，此外，这四种度量的取值范围都是[0, 1]，且含义都如下：
　　度量值(A, B){█(=0.5，A和B中性关联@>0.5，A和B呈正相关@<0.5，A和B呈负相关)┤
　　至此，除了支持度度量和置信度度量，我们已经介绍了6种新的评估方法。那么这些方法孰优孰劣呢？下面我们将通过特殊的案例进行说明，并比较这6种评估方法的性能。
　　让我们依旧以喝咖啡和喝绿茶为例，爱喝咖啡的人群以c（coffee）表示，不爱喝咖啡的人群则以c ̅表示，同理，爱喝茶和不爱喝茶的人群分别以t（tea）和t ̅表示。而既爱喝咖啡，又爱喝绿茶的人群就可以用ct表示，其他情况同理。
　　假设我们调查得到一个数据集，其中ct=c ̅t=ct ̅=100，(ct) ̅=10000。此时可以计算得到如下表所示结果。
　　表18-10 不同评估方法的度量值
　　
　　不难发现，对于该数据，提升度和不^2都认为爱喝茶和爱喝咖啡之间具有强正相关，而另外四种评估方法都认为爱喝茶和爱喝咖啡之间是中性关联。那么谁的判断比较正确呢？让我们详细分析一下。首先，观察该数据集，样本总数为10300，但是其中(ct) ̅的样本有10000，显著地大于另外三类样本数。而提升度和。^2都对(ct) ̅样本十分敏感，所以产生了不稳定的结果。在很多实际数据中，类似(ct) ̅这类不包含任何我们感兴趣的项的样本通常都远大于其他样本，所以我们期望的度量应该能不受此类样本影响，从而给出稳定且符合常理的结果。那为何全置信度、最大置信度、Kulc度量和余弦度量的结果都是0.50呢？因为在这个数据集中，ct=c ̅t=ct ̅=100，即ct/(c ̅t)=ct/(ct ̅ )=1，这说明如果一个人喜欢喝绿茶，那他同时还喜欢喝咖啡的概率是50%，如果他喜欢喝咖啡，那他还喜欢喝绿茶的概率也是50%，而这四种度量都没有受到(ct) ̅样本的影响，故正确地显示了两者呈中性关联。
　　让我们更准确地阐述后四种度量都具备的这种实用的性质。如(ct) ̅这类样本，我们将其称为零事务，即不包含任何用户感兴趣的项集的事务，许多大型事务数据库中，零事务的个数可能远远超过其他事务的总和，而如果一种评估方法能不受零事务个数的影响，即度量值独立于零事务个数，那么我们称该评估方法具有零不变性。由于我们探索的是用户感兴趣的项集之间的关联规则，所以在大型数据集中，我们通常更倾向于使用具有零不变性的评估方法，相比于提升度和。^2这类不具有零不变性的度量，前者对识别出有意义的关联规则更有帮助。
　　而这些零不变性度量之间，又是孰优孰劣呢？[l8]
　　
18.4 多维度量化关联规则挖掘
　　前面我们所介绍的是单维度布尔型关联规则挖掘，而在实际应用中，我们获得的大都是更加复杂的数据，涉及的挖掘方法也更多样。在这一小节，我们将向你介绍多维度关联规则挖掘和量化关联规则挖掘的基本概念和常见方法，使你对高级模式挖掘产生基本的认识。
　　
18.4.1多维度关联规则挖掘 
　　关于“什么是多维关联规则”这个问题，我们在18.1.2小节中已经作了介绍。简单来说，规则中涉及的谓词多于一个，便是多维关联规则。例如，若我们想表达如下关联关系“深圳的35岁以上的教师倾向于购买茶壶”，则可以书写为：
　　location(X, "Shenzhen" )    age(X, ""≥35\"" )    occupation(X, "teacher")→buys(X, "teapot")
　　该关联规则涉及4个谓词，这4个谓词{location, age, occupation,  buys}则构成一个4-谓词集，更进一步地说，这4个谓词都是不重复的，所以该关联规则是维间关联规则，即具有不重复谓词的多维关联规则。而包含重复谓词的多维关联规则被称为混合维关联规则，修改上例，关联关系“深圳的35岁以上的人群中购买了茶叶的倾向于购买茶壶”可表示如下：
　　location(X, "Shenzhen" )    age(X, ""≥35\"" )    buys(X, "tea")→buys(X, "teapot")
　　单维关联规则的挖掘对象是事务数据，而多维关联规则的挖掘对象则是更加复杂庞大的数据仓库或关系数据库等。在数据库中，一条记录的属性可能是标称的或量化的。标称属性的取值范围是有限多个可能值的集合，且值之间是无序的，如occupation、location就是标称属性。量化属性的值则是数值的，且值之间存在隐序，如age、income等属性。在多维关联规则挖掘中，我们搜寻的不再是频繁项集，而是频繁谓词集，频繁k-谓词集的集合同样以L_k表示。
　　多维关联规则的挖掘方法可以根据它对量化属性的处理方式分为三种，下面我们将一一介绍，但不会涉及到方法的具体细节。
 使用量化属性的静态离散化挖掘多维关联规则
　　即使用预定义的概念分组对量化属性离散化。在进行挖掘之前，我们首先使用自定义的分组替换掉原本的量化属性。比如，对于数值属性，我们可以通过预先划分的区间值，如[10, 20),[20, 30), [30, 40)等，将原本的数值属性转换为相应的区间，每个区间包含区间标号，可以看作一个分组，这样，就能以处理标称属性的方式对量化属性进行处理了。这种离散化是静态的，必须在挖掘之前进行。
 挖掘量化关联规则
　　即根据数据的分布情况，将量化属性离散化或聚类到“箱”。和第一种方法不同，该方法将数值属性的值处理为数量的形式，而非类别或区间。且这种离散化是动态的，在数据挖掘过程中可以进行自动调整，比如相邻的箱可能合并。自动调整的标准则是预定义的，比如最大化所得关联规则的提升度。ARCS（Association Rule Clustering System）就应用了该方法，它将量化属性对映射到满足给定的分类条件的2D栅格上，然后使用聚类算法搜索栅格点，从而得到关联规则。
 挖掘基于距离的关联规则
　　该方法同样是使用区间对量化属性离散化，但它不允许数据值的近似，这一方法考虑到了数据点之间的距离，因此得名，且应用在了两趟扫描算法中。即在第一趟扫描时，使用聚类算法寻找出簇，第二趟扫描搜索簇组，以得到基于距离的关联规则。
	
　　需要注意的是，我们介绍的上述方法可以用于多维关联规则挖掘，但并不适合挖掘高维数据，即维度达到数百甚至数千的数据。如今虽然有一些方法可以用于挖掘高维数据，如基于行枚举的模式增长方法，以及挖掘巨型模式的模式融合方法，但是仍有很长的路要走，我们在此则不作详细介绍。

18.4.2量化关联规则挖掘
　　量化关联规则的定义在18.1.2小节中已经介绍过，即包含了量化的项或属性的规则。由于量化关联规则和多维关联规则之间本身是有交叉的，正如我们在18.2节中挖掘的单维布尔型关联规则，我们现在讨论的是挖掘多维量化关联规则，所以两者的挖掘方法不必完全区分。而如18.4.1节介绍的使用量化属性的静态离散化挖掘方法，我们先将量化属性离散化到多个区间，从而作为标称数据看待，但这种做法可能导致产生大量冗余的无意义的关联规则。为了改善这一缺点，我们接下来会介绍一些新的方法，帮助我们能更高效地发现有趣的关联规则。
　　第一种方法是基于数据立方体进行挖掘。
　　第二种方法是基于聚类进行挖掘。
　　第三种方法则是基于统计学理论来发现能揭示异常行为的量化关联规则。
　　
　　
　　
18.5 时序关联规则挖掘
　　在生活中，数据事件之间常常存在时间或空间上的序数关系。在之前讨论的关联规则挖掘中，我们没有考虑数据中可能携带的序列信息。以前面的购物篮数据为例，我们只考虑了顾客单次购买的商品之间的联系，而对于一位多次光临的顾客，其每次购买的清单之间也可能包含一些有意义的关联模式，值得我们探究。实际上，许多数据在时间维度上都会呈现出某种趋势，比较直观的如股票数据和心电图数据等，我们不能忽视时间信息，而应该对其善加利用，这一节我们就将介绍时序关联规则的挖掘过程。
18.5.1时序模式的定义
　　在之前介绍的关联规则的基础上，引入时间或空间维度，便是序列模式，而这一节我们主要介绍引入时间维度的数据挖掘，所以称为时序模式。接下来，我们首先需要了解一些基本概念，对序列信息有一个清晰的认识。
　　18.1.3节中的事务数据库中，每一行是一个事务，每个事务有一个TID号和对应的项集。而一个序列则是事务的有序列表，通常记为s=<t_1 t_2 t_3 〖…t〗_n>，其中t_i表示一个事件集，这里的事件即相当于项。在讨论序列时，为了避免混淆，我们通常称事务为元素，称项为事件，即元素是事件的集合，序列是元素的有序列表。下面给出几个序列的例子。
　　（1）、某顾客在附近超市一个月的购买清单记录：
　　<{牛奶，面包}{矿泉水}{薯片，方便面}{衣架}{薯片}{清洁球}>
　　（2）、某校计算机科学专业学生不同学年参与的课程序列：
　　<{高等数学，线性代数，数据结构}{概率论，算法导论，离散数学}{操作系统，数据库系统}{计算机体系结构，编译原理，软件工程}>
　　我们通常以序列的长度来描述一个序列，序列长度即序列中元素的个数，其中包含k个元素的序列称为k-序列。在上述两个序列中，序列（1）有6个元素，7个事件，是一个6-序列；序列（2）则有4个元素，11个事件，是一个4-序列。在书写时，元素通常用{}包盖，而序列通常用<>包盖。需要注意的是，我们不考虑同一个事件的数量，故一个序列的同一个元素中不会出现重复的事件，但一个序列的不同元素中可以出现相同的事件。如，某次购物中，一顾客在超市先购买了1盒牛奶，再购买了2块面包，我们不会将其记为{牛奶，面包，面包}，而是记为{牛奶，面包}。
　　此外，我们还需要定义序列之间的包含关系。与项集中的包含关系略有不同，序列之间的包含关系略微复杂一些，具体如下：
　　对序列T=<t_1 t_2 t_3 〖…t〗_n>和序列S=<s_1 s_2 s_3 〖…s〗_m>，其中n≤m，如果存在整数1≤j_1<j_2<…〖<j〗_n≤m，使得t_i⊑s_(j_i )，i=1,2,…,n，那么序列T就是序列S的子序列。下面给出一些简单的示例。
　　
　　
18.5.2时序模式的基本框架
　　在时序数据中，我们要挖掘的是频繁序列。和挖掘频繁项集类似，我们首先需要定义一个最小支持度阈值（minsup）。对于一个序列来说，它的支持度是序列数据库中包含该序列的序列所占的比例，如果它的支持度大于设定的最小支持度阈值，就称其为频繁序列。
　　表18-11 一个序列数据库示例
　　
　　在该序列数据库中，每一行是一个序列，每个序列包含多个事件集。假设最小支持度阈值为50%，暂且列出部分候选序列的支持度如上右图所示。对于序列<{b}{c}>，它是S001、S003、S004的子序列，所以支持度为60%，需要注意的是，它并不是S002的子序列，因为时序数据本身还包含时间信息，在S002中，元素{a, c}出现在{b, g}之前，所以<{c}{b}>是S002的子序列，而<{b}{c}>则不是。对于序列<{c, g}>，它是S003的子序列，而并不是S002的子序列，同样是因为时间信息的冲突，<{c, g}>表示事件c和事件g出现在同一元素中，即两者同时出现，而在S002中，事件c则先于事件g出现，故<{c, g}>不是S002的子序列。
　　下面，我们首先来尝试枚举法，即列出全部的候选序列，然后一一计算它们的支持度，与最小支持度阈值比较，从而搜索出全部的频繁序列。
　　不过候选序列和候选项集的枚举有一些不同，具体如下。
　　首先，在项集中不会出现重复的项，但是在序列中，一个事件可以重复出现多次。比如，对于项a和项b，可以产生一个候选2-项集{a, b}，但是却可以产生多个候选2-序列，如<{a}{a, b}>，<{b, a}{b}>，<{a}{a}>，<{a}{b}>，<{a, b}{b, a}>等。
　　其次，项集忽视了序列信息，而在序列中，事件出现的先后次序是很重要的。比如，{a, b}和{b, a}表示的是相同的项集，但是<{a, b}>和<{b, a}>却是两个不同的序列。
　　由于这些不同，在使用枚举法时，候选序列的个数将十分庞大，其数量会随原始数据中事件数量的增加而呈现指数式增长，所以，想要使用枚举法来挖掘频繁序列，是十分困难的。我们希望你能了解使用枚举法挖掘频繁序列的基本过程，能够明白如何列举候选k-序列，并学会计算序列的支持度，但我们极力不推荐你使用枚举法，即使数据中的事件数量不多。
　　下面，我们将介绍三种用于频繁序列挖掘的算法，它们能帮助我们避免繁杂的枚举，有效减少候选序列数量，从而加速挖掘。

18.5.3 基于Apriori的算法
　　Rakesh Agrawal和Ramakrishnan Srikant于1995年提出了两种基于Apriori的序列挖掘算法，分别是AprioriAll和AprioriSome，下面我们将分别介绍并比较。
　　首先，我们需要介绍一些基本概念。在本节中，我们称频繁1-序列中的项集为Large Itemset，简写为litemset。频繁序列则被称为Large Sequence，Large Sequence中的元素都是litemset。此外，这一节介绍的两种算法的目的不是挖掘全部的频繁序列，而是挖掘全部的maximal sequence。那么什么是maximal sequence呢？我们定义在频繁序列的集合中，如果一个序列不是集合内任何其他频繁序列的子序列，这个频繁序列就是maximal sequence。（后面例子理清楚了，再补上例子描述）
　　下面我们来介绍算法的基本框架。AprioriAll算法和AprioriSome算法都遵循这个框架执行。我们将序列挖掘分为5个阶段，具体如下。
　　
　　第一个阶段是Sort Phase。我们得到的原始数据库通常如（例子待补充）所示，我们需要将其转换为序列数据库，以表18-12为例，以顾客ID为主键，以交易时间戳为次键，将原始数据库排序后，同一个顾客的全部事务便转换为一条序列，事务按照时间戳从小到大排列，如表18-13所示。
　　表18-12 某商场用户购物数据库
　　
　　
　　
　　表18-12 序列数据库
　　
　　第二个阶段是Litemset Phase。在这个阶段，我们从上一步得到的序列数据库中，按照挖掘频繁项集的方法，设定最小支持度阈值，迭代寻找频繁1-项集，频繁2-项集等，需要注意的是，在序列数据库中，如果一个项集在同一个序列的不同元素中出现，支持度计数只增加一次，具体过程如图18-12所示。
　　
图18-12  生成全部频繁项集
　　这些频繁项集的集合就构成了litemset的集合，对于litemset，我们通常会将它们映射到连续的整数或字母上，这样不管项集中项的个数，它们都平等地被看作一个实体，便于表示和比较。我们选择将litemsets映射到连续的小写字母上，如表18-14所示。这里我们使用“（）”代替“{}”来表示项集，是为了在下一个阶段作区分。
　　表18-14 Large Itemsets及映射

　　第三个阶段是Transformation Phase。由于频繁序列中只包含litemset，所以在这一步中，我们要将之前得到的序列数据库作一些转换。如果一个元素中不包含任何litemset，就删除该元素；剩余的每个元素，都转换为这个元素所包含的全部litemset的集合。而如果一个序列中不包含任何litemset，就删除该序列，以表18-12数据为例，转换后的序列数据库如表18-15所示。需要注意的是，即使一个序列在这一步被删除，之后在计算支持度时，序列总数也不会改变，只是我们预先判断这个序列不会包含任何频繁序列，为之后的支持度计算步骤减少计算量。
　　表18-15 转换后的序列数据库
 
　　第四个阶段是Sequence Phase。在这个阶段，我们要利用从第三阶段得到的序列数据库，应用AprioriAll或AprioriSome算法，从中挖掘出全部的频繁序列（Large Sequences）。具体算法我们之后再详细介绍。
　　第五个阶段是Maximal Phase。这一阶段的作用是从全部的频繁序列中找出全部的maximal sequence。假设全部频繁序列中，最大的序列长度为n。我们首先遍历每个n-序列，删除集合中该n-序列的全部子序列，然后遍历集合中剩余的（n-1）-序列，同样从集合中删除其全部子序列，按照这个步骤，n逐渐递减，当n等于1时，结束遍历，此时，集合中剩余的全部频繁序列都是maximal sequence。（考虑给简短的伪代码）
　　现在让我们来详细描述Sequence Phase。这一阶段算法的基本思想和我们之前介绍的Apriori算法类似，即自底向上，通过多轮迭代来产生全部的频繁序列。第一轮时，由于我们已经得到了litemset的集合，将它们略作转换，便是1-频繁序列的集合，记为L_1。而后，从第二轮开始的每一轮，我们先通过上一轮得到的频繁序列集合，生成当前这轮的候选频繁序列，然后计算它们的支持度，与最小支持度阈值比较，得到本轮的频繁序列集合，然后就能继续下一轮迭代，当某轮的候选频繁序列集合或频繁序列集合为空集时，迭代结束，我们就得到了全部的频繁序列集合，进入下个阶段。
　　考虑到我们的最终目的是挖掘出全部的maximal sequence，这一阶段便出现了两种思路，分别是count-all和count-some。AprioriAll就是count-all类型的算法，即统计全部序列的支持度，产生全部频繁序列后，在maximal phase再将其中的non-maximal sequence剪枝。而count-some类型的算法AprioriSome则是将sequence phase和maximal phase合并，这类算法分为向前和向后两个阶段，目的是在挖掘过程中尽量避免统计non-maximal sequence，但代价是它可能会需要统计一些非频繁序列的支持度，这其中的平衡便决定了算法的性能。下面我们将依次介绍这两种算法。
　　AprioriAll算法和我们之前介绍的Apriori算法的基本流程一模一样，只是在生成候选序列和支持度计数两个过程的细节略有不同。假设我们现在得到了L_(k-1)，现在要产生候选序列集合C_k，其过程具体如下：（提供伪代码）（提供案例）
　　
　　需要注意的是，在序列中顺序很重要，所以<{a, b}>、<{a, c}>能生成<{a, b, c}>和<a, c, b>两个候选序列。
　　对于支持度计数过程，我们可以像Apriori算法一样扫描整个序列数据库，然后得到每个候选序列的支持度计数，但是这需要多次扫描序列数据库，换个角度来说，对于序列数据库中的每个序列s，我们都需要检查各个候选序列c是否被s支持，十分费时。
　　这里我们介绍一种新的计算支持度的方法，其目的是对于序列数据库中的每个序列s，可以不必检查每个候选序列c，减少需要统计的候选序列的数量，从而实现加速。
　　假设我们要统计C_k中全部序列的支持度，那么我们首先根据C_k来构建hash树，hash树的节点分为叶结点和内部结点，叶结点中存储的是候选序列的集合，而内部结点中存储的则是hash表。我们定义根结点的深度为1，深度为d的结点指向深度为d+1的结点。构建hash树时，我们依次读入候选序列，起初每个候选序列都放入叶结点，当叶结点的候选序列数达到我们设定的阈值时，该叶结点就需要分裂，假设该叶结点的深度为k，其中的候选序列就将其第k个litemset作为hash函数的输入，根据返回值散列到不同的叶结点中，原本的叶结点则变成内部结点，其中存储一个hash表。
　　构建完hash树后，我们再依次读入序列数据库中的序列，统计它们需要检查哪些候选序列。假设我们当前读入序列s，在根结点时，我们将序列s的每个litemset依次作为hash函数的输入，得到返回值，然后根据返回值定位到下一层的结点。此时，会出现两种情况。如果我们到达的是叶结点，那么该叶结点中的序列就是序列s需要检查的候选序列；而如果我们现在到达的是内部结点，假设我们是通过散列序列s的某个litemset（称为i）到达该内部结点的，那么在下一次散列时，我们只需要考虑s中包含i的元素的后一个元素中的litemset，并依次对它们进行散列即可，而不需要散列s中的全部litemset。将序列s散列完成后，我们就得到了序列s所需要检查的候选序列集合C_s，然后再一一检查，统计支持度即可。以相同的步骤处理序列数据库中的每个序列后，我们就得到了全部候选序列的支持度计数。
　　然后再根据候选序列的支持度计数，与最小支持度阈值比较，我们就能得到全部的频繁序列，再将这些频繁序列作为下一阶段Maximal Phase的输入即可。

图18-13  AprioriAll算法生成过程
　　然后我们还需要进行maximal phase，由于频繁4-序列只有<{a}{b}{c}{d}>，我们要从L_3、L_1和L_2中删除其全部子序列。此时L_3中只剩下<{a}{c}{e}>，我们再从L_1和L_2中删除其全部子序列。删除后，L_2中没有剩余序列，该阶段结束。最后我们得到全部的maximal sequence，具体为：<{a}{b}{c}{d}>、<{a}{c}{e}>、<{f}>、<{a, c}>、<{a, f}>、<{c, f}>、<{a, c, f}>。
　　接着，我们来介绍AprioriSome算法。该算法分为两个阶段，首先是向前迭代阶段，这个阶段只挖掘特定长度的序列；然后是向后迭代阶段，这个阶段是挖掘出剩余的序列。比如，我们可能在向前迭代阶段挖掘了长度为1、2、4、6的频繁序列，然后在向后迭代阶段挖掘长度为3和5的频繁序列。为了确定在向前迭代阶段中，下一轮所要挖掘的序列长度，我们定义了一个next函数，该函数以本轮挖掘的序列长度为输入，并输出下一轮所要挖掘的序列长度。设输入参数为k，即本轮挖掘得到了长度为k的频繁序列，我们令〖hit〗_k=(|L_k |)/(|C_k |)，即在长度为k的候选序列中，频繁k-序列所占的比例。而通常我们的直觉是，〖hit〗_k越大，由L_k迭代产生的候选序列中非频繁序列越少，在非频繁序列上的统计开销就会少于在non-maximal sequence上的统计开销，就值得我们跳过更多的长度，所以我们给出next函数如下：
　　
　　设置next函数的作用就是在统计非频繁序列和统计non-maximal序列的开销之间寻求一个最佳的平衡点。如果不论k是多少，next(k)总是返回k+1，此时就是选择统计全部的non-maximal序列，AprioriSome算法就等价于AprioriAll算法。相反，如果next(k)=100×k，即返回一个与输入相差很大的数，那么就相当于选择统计大量的非频繁序列，是另一种极限情况。由于我们可能跳过某几个长度的序列，所以在第k轮迭代时，我们不一定得到了L_(k-1)，此时就可以通过C_(k-1)来生成C_k，因为L_(k-1)⊑C_(k-1)，所以可以保证生成的C_k不遗漏任何频繁序列。AprioriSome算法生成候选序列的规则和AprioriAll算法相同，k从2开始逐渐递增，只有当k和next(k)的返回值相等时，才会计算当前C_k的支持度，从而得到L_k，否则，不需要对C_k进行支持度计数。
　　当某一轮迭代生成的候选序列或频繁序列集合为空时，向前迭代阶段就终止，进入向后迭代阶段。假设我们已经挖掘了长度为1、2、4、6的频繁序列，此时转入向后迭代阶段，我们就要挖掘长度为5和3的频繁序列，此时我们已经得到了L_6，由于我们的目标是maximal sequence，所以C_5中是L_6中某个序列的子集的序列都可以被剪枝，同理，L_4、C_3、L_2、L_1中的子序列也都直接被剪枝。L_6的子序列都被剪枝后，我们再根据L_5进行剪枝，即序列长度递减，不断剪枝，最后留下的就都是maximal sequence。
　　下面我们给出AprioriSome的伪代码，具体如下。
　　
　　
　　
　　
　　
　　
　　
　　
　　
　　以表18-15中的数据为例，我们简单假设next(k)=2k，所以AprioriSome算法先生成C_1和L_1，然后生成C_2和L_2，这一部分过程和AprioriAll算法基本一致，如图所示：

图18-14  AprioriSome算法生成L_1和L_2
　　接着会根据L_2生成C_3，并跳过L_3，直接生成C_4与L_4，具体如图18-15所示。我们在生成C_3时，只通过先验原理对其进行剪枝，由于不需要生成L_3，所以不用对C_3进行支持度计数。
　　
图18-15  AprioriSome算法生成C_3和L_4
　　由于L_4生成C_5时，得到空集，所以向前阶段结束，下面我们进行向后阶段。首先，检查L_4，由于只有一个序列，所以其必定是maximal sequence，无需删除。然后，我们从C_3、L_1和L_2中找到<{a}{b}{c}{d}>的子序列并删除，结果如图18-16所示。然后再将C_3中剩余的序列进行支持度计数，根据最小支持度计数筛选得到<{a}{c}{e}>，再从L_1和L_2中删除该频繁3-序列的子序列，结果如图18-17所示。由于频繁2-序列中没有maximal sequence，所以我们已经得到了全部的maximal sequence，这与AprioriAll算法得到的结果相同。
　　
图18-16  AprioriSome算法后退阶段（1）
　　
图18-17  AprioriSome算法后退阶段（2）
　　
　　
18.5.4 GSP算法
　　在实际应用中，我们常常需要对序列模式加上一些时限约束，以助于其更好地反映元素之间的关联。比如，给定如下购物序列：<{洗衣机}{方便面}{啤酒}>，其中洗衣机和方便面的购买时间相隔一年，由于时间间隔较长，我们很难认为一年前购买的洗衣机会对一年后购买的方便面有影响或联系，因此，我们需要再原先的序列模式定义之上，引入时限约束，从而定义新的序列模式。GSP算法便是针对这种新的序列模式所提出的挖掘算法。
　　下面我们首先来介绍GSP算法中对序列模式的约束。
 最大间隔和最小间隔约束
　　这是通过约束序列中两个连续元素之间的时间差来限制序列模式。若设定最大时间差（maxgap）为3，则当前元素中的事件和前一个元素中的事件的间隔不能超过3，而设定最小时间差（mingap）为0，则意味着当前元素中的事件必须在前一个元素中的事件之后立即出现。下面我们设定maxgap=4，mingap=2，给出子序列是否满足最大间隔和最小间隔约束的案例。
　　
　　需要注意的是，由于加入了时限约束，在进行支持度计数时，我们还需要检查候选序列是否满足时限约束。此外，先验原理可能不再成立。假设序列数据库中某条序列为<{a}{b}{c}>，当没有时限约束时，序列<{a}{c}>和<{a}{b}{c}>都被该序列支持，此时加入时限约束maxgap=1，mingap=0，那么序列<{a}{c}>就不再被支持，而序列<{a}{b}{c}>依旧被支持，这就可能导致序列<{a}{b}{c}>的支持度超过其子序列的支持度，与先验原理相悖。之后我们会来解决这个问题。
 窗口大小约束
我们定义一个窗口大小阈值（window size），通常简写为ws。它用于指定一个序列中的任意元素中的事件最早和最晚出现的最大允许时间差。比如，当窗口大小为0时，同一元素中的全部事件必须同时出现，而当窗口大小为1时，子序列中某个元素里的多个事件可以在原数据库序列中的两个相邻元素内出现。下面我们设定ws=2，给出如下案例。

需要注意的是，如果在ws=2的基础上，我们再加入maxgap=3的限制，那么表格中第四行的子序列<{a, b, c}{e, f, g}>就不再被s支持，因为其最大时间差为事件a和g的事件差5，不满足maxgap的限制。
最大间隔和最小间隔约束是加大了对频繁序列的限制，而窗口大小约束则是在一定程度上放松了对频繁序列的判定，能帮助我们避免错过一些有趣的序列模式。
 分类标准约束
分类标准约束是加入了对事件之间纵向关系的考量。比如，事件a是事件b的上层事件，且事件b属于元素e，那么事件a也属于元素e。不过，在之后的讨论中，我们不会考虑分类标准约束，而只考虑前面的两种时限约束。
介绍完了对序列模式的约束，下面我们开始介绍GSP算法。
首先，我们需要重新定义k-序列。之前，我们定义包含k个元素的序列为k-序列，而在GSP算法中，我们定义包含k个事件的序列为k-序列，对于序列<{a, b}{c}>，按之前的定义，它是2-序列，而现在它是3-序列。而对于序列<{a, b}{a, c}>，按之前的定义，它是2-序列，现在它则是4-序列。即同一个事件出现在不同的元素中时，需要重复计数。关于这么定义的理由，我们会在之后阐述。
由于引入最大间隔和最小间隔约束后，先验原理不再满足，为了解决这一问题，我们现在给出邻接子序列的定义，从而改进先验原理。
定义18.4 邻接子序列 给定一个序列s=<s_1 s_2…s_n>和一个子序列c，如果下列条件之一成立，那么就称c为s的邻接子序列（contiguous subsequence）。
 c是从s中的s_1或s_n中删除一个事件后得到的。
 c是从s中至少包含两个事件的元素里删除一个事件后得到的。
 c是t的邻接子序列，而t是s的邻接子序列。
下面我们给出一个案例，帮助更好地理解邻接子序列的概念。

利用邻接子序列，我们可以得到如下形式的先验原理：如果一个k-序列是频繁的，则它的所有邻接(k-1)-子序列一定是频繁的。
如果一个序列s支持序列模式t，那么当没有最大间隔约束时，序列s也一定会支持t的所有子序列；而若有最大间隔约束，序列s则一定会支持t的所有邻接子序列。所以上述先验原理是成立的。
GSP算法的整体框架和Apriori算法类似，第一轮扫描原始序列数据库，得到全部1-序列的支持度，再根据设定的最小支持度阈值筛选出频繁1-序列。之后每轮将迭代地产生新的候选k-序列，然后利用先验原理剪枝，接着对剩余候选序列进行支持度计数，得到每轮的频繁k-序列。但是，GSP算法的候选生成过程和支持度计数过程与Apriori算法不同，下面我们将进行详细描述。
首先是候选生成过程。这个过程分为两个阶段，具体如下：
	第一步是连接阶段，即通过连接频繁(k-1)-序列来生成候选k-序列。假设有两个k-序列s_1和s_2，如果从s_1中去掉第一个事件得到的子序列与从s_2中去掉最后一个事件得到的子序列相同，那么s_1和s_2是可以合并的。在可以合并的情况下，如果s_2的最后一个元素只有一个事件，那么将s_2的最后一个元素添加到s_1的末尾，就得到一个候选k-序列；而如果s_2的最后一个元素有多个事件，则将s_2的最后一个事件添加到s_1的最后一个元素的末尾，同样得到一个候选k-序列。需要注意的是，如果令s_1=<{a}>，s_2=<{b}>，它们之间连接可以得到2个候选2-序列，即<{a, b}>、<{a}{b}>。不难发现，s_1和s_2都是它们连接得到的候选序列的邻接子序列。
	第二步是剪枝阶段，即利用先验原理对候选序列进行剪枝。当有maxgap时限约束时，如果一个候选序列的任何一个邻接子序列不满足最小支持度，就将其剪去。如果没有maxgap时限约束，我们需要检查候选序列的全部子序列，如果其中任何一个子序列不满足最小支持度，就将该候选序列剪去。可以看到，利用邻接子序列定义的先验原理，可以在一定程度上帮助我们减少需要检查的子序列的数量，从而加速剪枝。
接着是支持度计数过程。这一过程和AprioriAll算法中的支持度计数过程相似。首先利用hash树结构存储候选序列，然后利用hash树来减少序列数据库中每个序列需要检查的候选序列数量，再一一检查筛选后的候选序列是否被支持。在GSP算法中，支持度计数过程的前两步和AprioriAll算法完全一样，不同之处是它提出了一种高效的方法，用于检查一个特定的候选序列是否被某个序列支持。下面我们将作详细介绍。
假设s是序列数据库中的一个序列，c是一条候选序列，我们的目的是快速检查s是否支持c，即检查c是否为s的子序列。为了实现这一点，给出算法如下，它也分为向前迭代阶段和向后迭代阶段，但是和AprioriSome中的完全不同。
在向前阶段，我们在s中寻找c的相邻元素的出现，并检查它们之间的时间差是否满足时间约束，如果满足，则按顺序继续寻找c的下一个元素；如果不满足，就转入向后阶段。
假设在寻找过程中，c_i（出现时间为t）和其前一个元素c_(i-1)的出现时间不满足时间约束，从而转入了向后阶段。在向后阶段中，我们将c_(i-1)删去，在t-maxgap的时间点之后继续寻找新的c_(i-1)。接着可能出现以下三种情况，需要分别处理。
若找到后，两者的时间差满足时间约束，我们还不能马上转回向前阶段。由于c_(i-1)的出现时间改变了，我们还需要检查c_(i-1)和c_(i-2)的时间差是否依旧满足时间约束，若不满足，则还需要如调整c_(i-1)一样尝试调整c_(i-2)，直到当前全部元素之间的时间差都满足时间约束，才能转入向前阶段。
若没找到新的c_(i-1)，则说明s不支持c，迭代结束。
若找到后，但需要不断向前调整元素，最后如果导致c中的第一个元素c_1也需要重新寻找新的出现位置时，我们如果能找到新的c_1，就立即转回向前阶段，否则说明s不支持c，迭代结束。
每次从向后阶段转回向前阶段时，假设向后阶段中最后一个需要调整出现位置的元素是c_j，回到向前阶段时，我们就需要在s中从c_(j+1)继续寻找。
向前阶段和向后阶段是交替进行的，如果在s中找到了c的全部元素，且都满足时间约束，那么就说明s支持c。下面我们结合一个具体案例进行说明。
以图18-13（1）中数据为序列数据库中的一条序列数据s，设maxgap=30，mingap=5，window-size=0，我们要检查的候选序列p为<{a, b}{c}{d}>。我们首先在s中找到p中第一个元素{a, b}的出现时间为10，然后找到p中第二个元素{c}的出现时间为45，两者的时间差为35，大于maxgap，所以转入向后阶段。删去{a, b}，在时间点15（即t_({a, b})-maxgap）之后继续寻找新的{a, b}出现时间。我们找到新的{a, b}出现时间为50，由于{a, b}是p中第一个元素，不用考虑调整{a, b}出现时间后，其与之前元素的时间约束是否还满足的问题。然后转回向前阶段，从时间55（t_({a, b})+mingap）之后重新寻找{a, b}的下一个元素，即{c}。{c}的下一个出现时间是65，由于65-50=15，满足时间约束。接着我们从时间70（t_({c})+mingap）开始继续寻找下一个元素{d}，{d}的出现时间是90，由于90-65=25，满足时间约束，且p中的全部元素都已找到，所以迭代结束，得出结论序列s支持候选序列p。
在上面的流程中，我们没有详细描述“在序列中找到某个元素的出现时间”这一过程，事实上，如果我们每次都通过遍历序列来寻找给定元素的出现时间的话，效率将严重下降。而若将数据序列s的表示方法作如图18-13（2）所示变换，就能这一过程更加高效。

     图18-13  （1）为序列数据，右为序列数据转换后的表示
创建一个字典，其中键是序列s中的事件名称，值则是该事件在序列s中的出现时间，具体如图18-13（2）所示。要找到事件q在时间节点t之后的第一次出现时间，我们只需要遍历字典中q对应的值，然后找到比t大的第一个值即可。创建这个字典的时间复杂度是O(n+m)，其中n是整个序列数据库中事件的个数，m则是序列s中事件的个数。
对于包含多个事件的元素，设元素中最早和最晚的事件的出现时间分别为start-time（st）和end-time（et），如果两者之差大于设定的window-size（ws），我们需要从t=et-ws开始，重复上述搜索过程；否则就结束搜索。
仍旧以图18-13（1）中所示，设window-size=7，如果我们需要搜索t=20之后元素{b, f}的第一次出现时间，则过程如下。首先，根据图18-13（2），我们找到b第一次出现时间是50，而f的第一次出现时间是25，故et-st=50-25=25>7，然后设置t=et-ws=50-7=43，从时间点t往后重新搜索。我们找到b的出现时间仍是50，而f的出现时间为95，此时et-st=45>7，再设置t=et-ws=88，重新搜索。这次，我们找到b的出现时间是90，f的出现时间仍为95，此时et-st=5<7，搜索结束。
最后让我们总结一下GSP算法。它加入了maxgap、mingap和window-size等时限约束，以期不会忽视一些有趣的序列模式。GSP算法的总体流程和Apriori类算法一致，主要分为两个步骤，一是候选序列生成，二是候选序列支持度计数。在候选序列生成中，GSP算法定义了邻接子序列，重新定义了连接阶段和剪枝阶段，最后输出候选序列。在支持度计数过程中，GSP算法通过hash树数据结构减少了数据库中单个序列需要检查的候选序列数量，并通过变换数据库中序列的表示方式，达到高效查询单个候选序列是否被数据库中序列支持的目的。
18.5.5 PrefixSpan算法
　　
18.5.6算法优缺点
　　
　　
18.6 关联规则挖掘的其它研究问题
　　
　　
　　
　　
　　 
　　
　　
　　
　　
　　
　　
　　

1 按照多维数据库使用的术语，通常将规则中的不同谓词称作维。
[l1]概念定义部分与书本语句有重复
[l2]和书本有相似
[l3]和书本有相似
[l4]在18.2.4里比较
[l5]不太明白，资料较少
[l6]例子考虑替换，有重复
[l7]参考文献
[l8]看情况，待补充，不平衡比等信息
---------------

------------------------------------------------------------

---------------

------------------------------------------------------------

